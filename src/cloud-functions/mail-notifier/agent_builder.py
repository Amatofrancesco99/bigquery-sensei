import os
import json
import logging
import requests
import random
from bs4 import BeautifulSoup
import time
from urllib.parse import urlparse, parse_qs
from typing import List, Dict, Optional, Any, TypedDict, Tuple
from datetime import datetime, timezone
from langchain_google_vertexai import ChatVertexAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langgraph.graph import StateGraph, END
from pydantic import BaseModel, Field, ValidationError
from langchain_core.messages import AIMessage, BaseMessage

logging.basicConfig(
    level=os.getenv("LOG_LEVEL", "INFO"),
    format='%(asctime)s - %(levelname)s - %(message)s'
)


class AnalysisOutput(BaseModel):
    """
    Defines the expected structured JSON output format from the language model
    for a user's summarized report over a time window, based on the provided agent_config.json.

    This model represents the textual summaries, suggestions, and the *finally verified* URL
    suggestions generated by the agent after analyzing a collection of
    job analysis records for a single user over a specific time window and performing web search.
    It strictly adheres to the output schema defined in the agent_config.json, ensuring
    the final output structure for reporting remains consistent.

    Attributes:
        stats_summary_extraction: A detailed textual summary analyzing the user's
                                  overall performance based on the provided job statistics
                                  over the specified time window. This summary highlights
                                  key trends, typical resource usage (like data processed),
                                  average quality/risk metrics across executed queries,
                                  and notable patterns, including improvements or
                                  worsening compared to previous time windows.
                                  This is generated by the initial LLM call.
        suggested_areas_of_improvement: Actionable insights identifying specific
                                        skills or categories of SQL query writing
                                        (e.g., Cost Optimization, Performance Tuning,
                                        Data Volume Efficiency, Readability) where the
                                        user's aggregated statistics indicate the most
                                        significant opportunities for development.
                                        This field's content, generated by the initial LLM call,
                                        is used to guide the subsequent web search for resources.
        online_study_urls: A curated list of external online resources (e.g., articles,
                           tutorials, documentation, best practice guides) specifically
                           relevant to the suggested areas of improvement, found through
                           web search, evaluation, and final selection for diversity.
                           This list is populated by the search, evaluation, and
                           finalization nodes in the LangGraph workflow and is capped
                           at a maximum of 4 URLs.
    """
    stats_summary_extraction: Optional[str] = Field(None, description="Detailed summary of user's performance over the time window, and past time windows.")
    suggested_areas_of_improvement: Optional[str] = Field(None, description="Summary of areas where the user needs to improve.")
    online_study_urls: List[str] = Field([], description="List of specific search sources to study.")


def validate_url(url: str) -> bool:
    """
    Checks if a given URL is accessible and returns a success status code.

    Performs an HTTP HEAD request to the URL to check its existence and status
    without downloading the full content. URLs returning 2xx (Success) or
    3xx (Redirection) status codes are considered valid. This function is used
    as a final accessibility check for URLs identified through the web search
    and evaluation process before including them in the final report.

    Args:
        url: The URL string to validate.

    Returns:
        True if the URL is accessible and returns a successful or redirection
        status code, False otherwise or if an error occurs during the request.
    """
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
        response = requests.head(url, timeout=5, headers=headers)
        return 200 <= response.status_code < 400
    except requests.exceptions.RequestException as e:
        logging.debug(f"URL validation failed for {url}: {e}")
        return False


def load_agent_config(config_path: str) -> Dict[str, Any]:
    """
    Loads, validates, and returns the agent configuration from a JSON file.

    This function reads the agent's configuration from the specified JSON file.
    It performs basic validation to ensure the presence of essential keys like
    'instructions' and 'output_schema'. It is critical for setting up
    the LLM prompt and understanding the expected output format.

    Args:
        config_path: The absolute or relative file path to the JSON
                     configuration file.

    Returns:
        A dictionary representing the parsed JSON configuration.

    Raises:
        ValueError: If the provided config_path is empty or None, or if essential
                    keys are missing from the configuration.
        FileNotFoundError: If the configuration file cannot be found at the
                           specified path.
        json.JSONDecodeError: If the file content is not valid JSON.
        Exception: For any other unexpected errors during file reading or parsing.
    """
    if not config_path:
        logging.error("Configuration path argument is missing.")
        raise ValueError("Agent configuration file path is not provided.")
    if not os.path.exists(config_path):
        logging.error(f"Configuration file not found at: {config_path}")
        raise FileNotFoundError(f"Agent configuration file not found: {config_path}")

    try:
        with open(config_path, "r") as f:
            config = json.load(f)
            logging.info(f"Agent configuration loaded successfully from {config_path}")
            if "instructions" not in config or "output_schema" not in config:
                 raise ValueError("agent_config.json must contain 'instructions' and 'output_schema'.")
            return config
    except json.JSONDecodeError as e:
        logging.error(f"Failed to decode JSON from {config_path}: {e}")
        raise
    except Exception as e:
        logging.error(f"An unexpected error occurred loading configuration from {config_path}: {e}")
        raise


class GraphState(TypedDict):
    """
    Defines the dictionary structure for the state passed between LangGraph nodes.

    This state object holds all the necessary information for the user report
    summarization workflow to progress, including input data, intermediate
    results from the language model, search results, and the final output.
    Each instance of this state represents the summarization process for a single
    user over a specific time window. It carries information across nodes
    in the LangGraph workflow.

    Attributes:
        llm_input: A list of dictionaries, where each dictionary represents a
                   job analysis record from the query_results_table for the
                   specific user and time window. This is the primary input
                   to the LLM for summarization and suggestions.
                   Sensitive fields like user_email should be excluded before
                   passing this data to the LLM input prompt string.
        user_email: The email address of the user whose data is being summarized.
                    This is kept in the state for the final output but not passed
                    to the LLM input directly.
        window_start_time: The start timestamp (UTC) of the time window for the report.
                           Passed into the state from the main logic.
        window_end_time: The end timestamp (UTC) of the time window for the report.
                         Passed into the state from the main logic.
        total_bytes_billed: The sum of total_bytes_billed for all jobs in the window.
                           This is calculated before the agent is invoked and stored
                           in the state for the final output.
        analyzed_jobs_count: The count of jobs analyzed in the window. This is
                             calculated before the agent is invoked and stored
                             in the state for the final output.
        average_spider_scores: The average spider graph scores for the user's queries in the window.
        grade: The overall grade for the user's queries in the window,
               calculated before the agent is invoked (e.g., average of
               individual job grades). Stored in the state for the final output.
        llm_raw_output: An optional dictionary containing the raw parsed output
                        from the initial language model call. This dictionary
                        is expected to contain 'stats_summary_extraction' and
                        'suggested_areas_of_improvement' based on the AnalysisOutput model.
        key_topics: List[str] - A list of concise strings representing the key
                    improvement areas extracted from suggested_areas_of_improvement.
                    Populated by the extract_topics node.
        topics_to_process: List[str] - A mutable list used by the iterative search
                           workflow to keep track of topics that still need searches performed.
        current_topic: Optional[str] - The single topic being processed in the current
                       iteration of the search loop.
        current_search_query: Optional[str] - The search query generated specifically
                              for the current topic.
        cumulative_promising_results: List[Dict[str, str]] - A list accumulating
                                      promising search results (dicts with 'title',
                                      'snippet', 'link') found across all topic searches.
        validated_urls: List[str] - A list of strings, containing the final curated list of
                        relevant, diverse, and accessible online study URLs selected
                        for the user report, capped at 4 URLs. This is the final
                        output of the URL discovery process.
        final_formatted_output: Optional[Dict[str, Any]] - The final structured output
                                for the user report, formatted to match the target schema
                                (mail_notifier_suggestions_table schema).
        raw_llm: ChatVertexAI - The initialized Vertex AI language model instance
                 used for summarization, suggestion, and URL selection evaluation.
        llm_model: str - The name or identifier of the language model being used.
        output_parser: JsonOutputParser - The parser used to structure the initial
                       LLM's response into the AnalysisOutput format.
        _prompt: ChatPromptTemplate - The Langchain ChatPromptTemplate used
                 for the initial LLM call.
        output_schema_str: str - A string representation of the expected JSON
                           output schema for the initial LLM call.
    """
    llm_input: List[Dict[str, Any]]
    user_email: str
    window_start_time: datetime
    window_end_time: datetime
    total_bytes_billed: Optional[int]
    analyzed_jobs_count: Optional[int]
    grade: Optional[int]
    average_spider_scores: Optional[dict]
    llm_raw_output: Optional[Dict[str, Any]]
    key_topics: List[str]
    topics_to_process: List[str]
    current_topic: Optional[str]
    current_search_query: Optional[str]
    cumulative_promising_results: List[Dict[str, str]]
    validated_urls: List[str]
    final_formatted_output: Optional[Dict[str, Any]]
    raw_llm: ChatVertexAI
    llm_model: str
    output_parser: JsonOutputParser
    _prompt: ChatPromptTemplate
    output_schema_str: str


def json_serial(obj):
    """
    JSON serializer for objects not serializable by default json code.

    Handles datetime objects by converting them to ISO format strings,
    objects with a 'dict' or 'model_dump' method (like Pydantic models),
    Langchain BaseMessage objects (by serializing content), and
    specific Langchain/tool objects by returning a descriptive string
    to avoid serialization errors.

    Args:
        obj: The object to serialize.

    Returns:
        A JSON serializable representation of the object.

    Raises:
        TypeError: If the object's type is not serializable by this function
                   or the default json encoder.
    """
    if isinstance(obj, datetime):
        return obj.isoformat()
    if hasattr(obj, 'model_dump'):
        return obj.model_dump()
    if hasattr(obj, 'dict'):
        return obj.dict()
    if isinstance(obj, BaseMessage):
         return obj.content
    if isinstance(obj, ChatPromptTemplate):
         return "ChatPromptTemplate object"

    raise TypeError ("Type %s not serializable" % type(obj))


def build_prompt_template(instructions: str, output_schema_str: str, examples: List[Dict[str, Any]]) -> ChatPromptTemplate:
    """
    Builds the ChatPromptTemplate by constructing a single template string and
    escaping curly braces within literal JSON content to prevent misinterpretation
    by LangChain's formatter.

    Args:
        instructions: A string containing the system instructions for the LLM.
        output_schema_str: A string containing the JSON schema the LLM's output
                           must adhere to for the summary and suggestions part.
        examples: A list of dictionaries, where each dictionary contains 'input'
                  (a list of job analysis records) and 'output' (the desired
                  summarized report structure) keys representing example interactions.

    Returns:
        A ChatPromptTemplate object.
    """
    escaped_output_schema_str = output_schema_str.replace('{', '{{').replace('}', '}}')

    system_template_content = f"System: {instructions.strip()}\n\nOutput MUST be valid JSON matching:\n{escaped_output_schema_str.strip()}\n\nDo NOT include 'online_study_urls' in your initial JSON output; the agent will handle finding resources separately."

    example_template_content = ""
    if examples:
         logging.info(f"Processing {len(examples)} examples for prompt.")
         for i, example in enumerate(examples):
             example_input = example.get('input', {})
             example_output = example.get('output', {})
             curated_example_output = {k: v for k, v in example_output.items() if k != 'online_study_urls'}

             if example_input and curated_example_output:
                 human_example_json = json.dumps(example_input, indent=2, default=json_serial)
                 ai_example_json = json.dumps(curated_example_output, indent=2, default=json_serial)
                 escaped_human_example_json = human_example_json.replace('{', '{{').replace('}', '}}')
                 escaped_ai_example_json = ai_example_json.replace('{', '{{').replace('}', '}}')
                 example_template_content += f"\n\nHuman: Input Job Data Records:\n{escaped_human_example_json}\n\nAI: {escaped_ai_example_json}"
             else:
                 logging.warning(f"Skipping malformed or incomplete example during prompt setup: {example}")

    final_human_template_content = "\n\nHuman: Analyze these BigQuery job data records:\n{input_data}"
    full_template_string = system_template_content + example_template_content + final_human_template_content
    prompt = ChatPromptTemplate.from_template(full_template_string)

    logging.info("Prompt template defined using from_template with escaped JSON.")
    return prompt


def call_llm_summarize_suggest_node(state: GraphState) -> Dict[str, Any]:
    """
    LangGraph node that invokes the language model to generate the performance summary
    and suggested areas for improvement based on the user's job analysis records.

    This node takes the aggregated job data and the configured prompt, sends it
    to the LLM, and parses the JSON response. It is designed to extract only the
    summary ('stats_summary_extraction') and suggestions ('suggested_areas_of_improvement'),
    as URL discovery is handled later in the workflow.

    Args:
        state: The current state of the LangGraph, containing 'llm_input',
               'raw_llm', '_prompt', and 'output_parser'.

    Returns:
        A dictionary containing updates to the state: 'llm_raw_output' with the
        parsed summary and suggestions (or None if parsing fails), and initializes
        search-related fields ('key_topics', 'topics_to_process', 'cumulative_promising_results').
    """
    user_email = state.get('user_email')
    logging.debug(f"[Node: call_llm_summarize_suggest] - Summarizing data and suggesting improvements for user: {user_email}")
    raw_llm: ChatVertexAI = state.get('raw_llm')
    output_parser: JsonOutputParser = state.get('output_parser')
    llm_input_data = state.get('llm_input')
    prompt: ChatPromptTemplate = state.get('_prompt')
    average_spider_scores: dict = state.get('average_spider_scores')

    curated_llm_input = []
    for record in llm_input_data:
        curated_record = {k: (v.isoformat() if isinstance(v, datetime) else v) for k, v in record.items() if k != 'user_email'}
        curated_llm_input.append(curated_record)
        curated_llm_input.append({"average_spider_scores": average_spider_scores})

    input_data_for_prompt = json.dumps(curated_llm_input, indent=2, default=json_serial)

    try:
        formatted_prompt = prompt.format_prompt(input_data=input_data_for_prompt)
        messages_for_llm = formatted_prompt.to_messages()
        raw_llm_response: AIMessage = raw_llm.invoke(messages_for_llm)

        response_content = raw_llm_response.content

        if response_content.strip().startswith("```json"):
             response_content = response_content.strip()[len("```json"):].strip()
        if response_content.strip().endswith("```"):
             response_content = response_content.strip()[:-len("```")].strip()


        llm_output_parsed = None
        try:
            llm_output_parsed = output_parser.invoke(response_content)
            validated_output = AnalysisOutput(**llm_output_parsed)
            logging.debug(f"[Node: call_llm_summarize_suggest] - Parsed LLM output validated against schema for user {user_email}.")

            llm_raw_output_for_state = {
                "stats_summary_extraction": validated_output.stats_summary_extraction,
                "suggested_areas_of_improvement": validated_output.suggested_areas_of_improvement,
                "online_study_urls": []
            }

            return {
                "llm_raw_output": llm_raw_output_for_state,
                "key_topics": [],
                "topics_to_process": [],
                "cumulative_promising_results": [],
                "validated_urls": []
            }

        except ValidationError as e:
            logging.error(f"[Node: call_llm_summarize_suggest] - Parsed LLM output schema validation failed for user {user_email}: {e}", exc_info=True)
            return {"llm_raw_output": None, "key_topics": [], "topics_to_process": [], "cumulative_promising_results": [], "validated_urls": []}
        except json.JSONDecodeError:
            logging.warning(f"[Node: call_llm_summarize_suggest] - Could not JSON parse LLM response for user {user_email}. Content: {response_content[:500]}...")
            return {"llm_raw_output": None, "key_topics": [], "topics_to_process": [], "cumulative_promising_results": [], "validated_urls": []}
        except Exception as parse_e:
             logging.warning(f"[Node: call_llm_summarize_suggest] - An unexpected error occurred during JSON parsing for user {user_email}: {parse_e}. Content: {response_content[:500]}...", exc_info=True)
             return {"llm_raw_output": None, "key_topics": [], "topics_to_process": [], "cumulative_promising_results": [], "validated_urls": []}


    except Exception as node_e:
        logging.error(f"[Node: call_llm_summarize_suggest] - LLM invocation error for user {user_email}: {node_e}", exc_info=True)
        return {"llm_raw_output": None, "key_topics": [], "topics_to_process": [], "cumulative_promising_results": [], "validated_urls": []}


def extract_topics_node(state: GraphState) -> Dict[str, Any]:
    """
    LangGraph node that uses the LLM to extract distinct key topics/areas
    from the suggested_areas_of_improvement text.

    Takes the text generated by the initial LLM call for 'suggested_areas_of_improvement'
    and prompts the LLM again to identify and list the main topics or categories
    mentioned within that text, returning them as a JSON array of strings.
    This makes the search query generation more robust to variations in the
    LLM's formatting of the suggestions.

    Args:
        state: The current state of the LangGraph, containing 'llm_raw_output',
               'raw_llm'.

    Returns:
        A dictionary containing an update to the state: 'key_topics' with the
        list of extracted topic strings (cleaned and validated), or an empty
        list if extraction fails or no suggestions were available initially.
    """
    user_email = state.get('user_email')
    logging.debug(f"[Node: extract_topics] - Extracting key topics for user: {user_email}")
    llm_raw_output = state.get('llm_raw_output')
    raw_llm: ChatVertexAI = state.get('raw_llm')

    suggested_areas = None
    if isinstance(llm_raw_output, dict):
        suggested_areas = llm_raw_output.get('suggested_areas_of_improvement')

    if not suggested_areas:
        logging.warning(f"[Node: extract_topics] - No suggested areas found for user {user_email}. Cannot extract topics.")
        return {"key_topics": []}

    extraction_prompt_template = """
        Analyze the following text describing areas for improvement in data warehousing and querying practices, particularly within a BigQuery context.
        Identify the distinct key topics or categories mentioned, regardless of the formatting (e.g., numbered lists, bullet points, paragraphs).
        Return these topics as a JSON array of strings.
        For each topic, formulate a concise phrase suitable for a search query.
        **Important:** Prefix the topic with "BigQuery" ONLY if the concept or practice is specifically tied to BigQuery. If the topic is a general SQL or data warehousing concept applicable beyond BigQuery, use a more general term without the "BigQuery" prefix.

        Examples:
        - If the text mentions optimizing scan size in BigQuery, the topic should be "BigQuery Cost Optimization".
        - If the text discusses partitioning tables, and the context implies BigQuery partitioning, use "BigQuery Partitioning".
        - If the text mentions adding comments to SQL queries, the topic should be "Query Commenting" (as commenting is a general SQL practice).
        - If the text talks about choosing appropriate data types for performance, the topic might be "Data Type Selection" or "SQL Data Types Best Practices".

        Output ONLY the JSON array of strings.

        Text to analyze:
        {suggested_areas_text}

        Example output format:
        ```json
        ["Topic 1", "General Topic 2", "BigQuery Specific Topic 3"]
        ```
    """

    extraction_prompt = ChatPromptTemplate.from_template(extraction_prompt_template)
    messages_for_extraction = extraction_prompt.format_messages(
        suggested_areas_text=suggested_areas
    )

    key_topics = []
    try:
        extraction_response: AIMessage = raw_llm.invoke(messages_for_extraction)
        response_content = extraction_response.content

        if response_content.strip().startswith("```json"):
             response_content = response_content.strip()[len("```json"):].strip()
        if response_content.strip().endswith("```"):
             response_content = response_content.strip()[:-len("```")].strip()

        try:
            extracted_list = json.loads(response_content)
            if isinstance(extracted_list, list):
                 key_topics = [topic for topic in extracted_list if isinstance(topic, str) and topic.strip()]
                 logging.debug(f"[Node: extract_topics] - Successfully extracted {len(key_topics)} topics for user {user_email}.")
            else:
                 logging.warning(f"[Node: extract_topics] - LLM extraction response was not a list for user {user_email}. Type: {type(extracted_list)}. Content: {response_content[:500]}... Treating as empty.")

        except json.JSONDecodeError:
            logging.warning(f"[Node: extract_topics] - Could not JSON parse LLM extraction response for user {user_email}. Content: {response_content[:500]}...")
        except Exception as parse_e:
             logging.warning(f"[Node: extract_topics] - An unexpected error occurred during JSON parsing for topic extraction for user {user_email}: {parse_e}. Content: {response_content[:500]}...", exc_info=True)

    except Exception as node_e:
        logging.error(f"[Node: extract_topics] - Error during topic extraction LLM call for user {user_email}: {node_e}", exc_info=True)

    return {"key_topics": key_topics}


def initialize_topic_iteration_node(state: GraphState) -> Dict[str, Any]:
    """
    LangGraph node to initialize state variables for iterating through topics.

    Copies the list of key topics and initializes the cumulative list for
    promising search results found across all topic searches. If no key topics
    were extracted, it initializes with an empty list of topics to process,
    which will cause the router to immediately exit the loop.

    Args:
        state: The current state of the LangGraph, containing 'key_topics'.

    Returns:
        A dictionary updating the state with 'topics_to_process' (list of topics
        to iterate through) and 'cumulative_promising_results' (empty list).
    """
    user_email = state.get('user_email')
    logging.debug(f"[Node: initialize_topic_iteration] - Initializing topic iteration for user: {user_email}")
    key_topics = state.get('key_topics', [])

    return {
        "topics_to_process": list(key_topics) if isinstance(key_topics, list) else [],
        "cumulative_promising_results": []
    }


def topic_router_node(state: GraphState) -> str:
    """
    LangGraph conditional node (router) to check if there are more topics to process.

    Based on whether the 'topics_to_process' list in the state is empty,
    it directs the workflow either to process the next topic or to finish
    the iteration and proceed to finalizing URLs. This function is intended to be
    used as the 'condition' callable in `add_conditional_edges`.

    Args:
        state: The current state of the LangGraph, containing 'topics_to_process'.

    Returns:
        A string indicating the next node: "process_topic" to continue the loop,
        or "finish_iteration" to exit the loop.
    """
    user_email = state.get('user_email')
    topics_to_process = state.get("topics_to_process", [])

    if isinstance(topics_to_process, list) and topics_to_process:
        logging.debug(f"[Node Logic: topic_router_node] - More topics to process ({len(topics_to_process)} remaining) for user: {user_email}. Routing to process_topic.")
        return "process_topic"
    else:
        logging.debug(f"[Node Logic: topic_router_node] - No more topics to process for user: {user_email}. Routing to finish_iteration.")
        return "finish_iteration"


def process_next_topic_node(state: GraphState) -> Dict[str, Any]:
    """
    LangGraph node to get the next topic from the list to process.

    Pops the first topic from the 'topics_to_process' list in the state
    and sets it as the 'current_topic' in the state for the next steps
    in the iteration. Updates the 'topics_to_process' list in the state.

    Args:
        state: The current state of the LangGraph, containing 'topics_to_process'.

    Returns:
        A dictionary updating the state with 'current_topic' (the topic being
        processed in this iteration) and the updated 'topics_to_process' list.
        Returns None for 'current_topic' if no topics are left (should be
        prevented by the router, but handled defensively).
    """
    user_email = state.get('user_email')
    topics_to_process = state.get("topics_to_process", [])

    if not isinstance(topics_to_process, list):
         logging.error(f"[Node: process_next_topic] - topics_to_process is not a list for user: {user_email}. Type: {type(topics_to_process)}. Cannot process topic.")
         return {"current_topic": None, "topics_to_process": []}

    if topics_to_process:
        current_topic = topics_to_process.pop(0)
        logging.debug(f"[Node: process_next_topic] - Processing topic: '{current_topic}' for user: {user_email}. {len(topics_to_process)} topics remaining.")
        return {"current_topic": current_topic, "topics_to_process": topics_to_process}
    else:
        logging.warning(f"[Node: process_next_topic] - process_next_topic called but no topics remain for user: {user_email}. This suggests a router logic issue or graph error.")
        return {"current_topic": None, "topics_to_process": topics_to_process}


def generate_search_query_node(state: GraphState) -> Dict[str, Any]:
    """
    LangGraph node that generates a search query string for the current topic.

    Takes the 'current_topic' from the state (populated by the process_next_topic
    node) and combines it with a base phrase ("BigQuery") to form a concise
    search query string suitable for web scraping for this specific topic.

    Args:
        state: The current state of the LangGraph, containing 'current_topic'.

    Returns:
        A dictionary containing an update to the state: 'current_search_query' with the
        generated search string, or None if no current topic is available.
    """
    user_email = state.get('user_email')
    logging.debug(f"[Node: generate_search_query] - Generating search query for current topic for user: {user_email}")
    current_topic = state.get('current_topic')

    if not current_topic:
        logging.warning(f"[Node: generate_search_query] - No current topic available in state for user {user_email}. Cannot generate search query.")
        return {"current_search_query": None}

    query = f'BigQuery {current_topic}'
    query = query[:500]

    logging.info(f"[Node: generate_search_query] - Generated search query for topic '{current_topic}' for user {user_email}: {query}")

    return {"current_search_query": query}


def scrape_google_search_results(query: str, num_results: int) -> List[Dict[str, str]]:
    """
    Scrapes Google search results using requests and BeautifulSoup for a given query
    and attempts to return up to num_results.

    This function simulates fetching search results by making HTTP requests to
    Google Search and parsing the HTML response. It handles basic pagination
    to retrieve results across multiple pages until the desired number of
    results is met or no further pages are found.

    Args:
        query: The search query string.
        num_results: The maximum number of search results to attempt to retrieve.

    Returns:
        A list of dictionaries, where each dictionary represents a search result
        with 'title', 'snippet', and 'link'. Returns an empty list on failure
        or if no results are found.
    """
    logging.info(f"Attempting to scrape Google for query: '{query}' aiming for {num_results} results.")
    search_results = []
    start = 0
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36"
    }

    MIN_DELAY = 5
    MAX_DELAY = 15

    while len(search_results) < num_results:
        params = {
            "q": query,
            "hl": "en",
            "start": start,
        }

        try:
            logging.info(f"Fetching results from start={start} (current results: {len(search_results)})...")

            sleep_duration = random.uniform(MIN_DELAY, MAX_DELAY)
            logging.info(f"Sleeping for {sleep_duration:.2f} seconds...")
            time.sleep(sleep_duration)

            html = requests.get("https://www.google.com/search", params=params, headers=headers, timeout=20)
            html.raise_for_status()
            soup = BeautifulSoup(html.text, 'lxml')

            organic_results = soup.select(".tF2Cxc")
            if not organic_results:
                 organic_results = soup.select("div.g")
                 if not organic_results:
                    logging.info("No more organic results found on this page with primary or alternative selectors.")
                    break

            page_results_count = 0
            for result_container in organic_results:
                if len(search_results) >= num_results:
                    break

                title_element = result_container.select_one("h3")
                link_element = result_container.select_one("a")

                snippet_div = result_container.select_one('div[data-sncf="1"] > div')
                snippet = ""
                if snippet_div:
                     snippet = snippet_div.get_text(separator=" ", strip=True)
                else:
                    snippet_parts = result_container.select('span')
                    temp_snippet = []
                    for part in snippet_parts:
                         text = part.get_text(strip=True)
                         if len(text) > 30 and not part.find_parent('a'):
                             temp_snippet.append(text)
                    snippet = " ".join(temp_snippet)


                title = title_element.text if title_element else None
                link = link_element["href"] if link_element and link_element.has_attr("href") else None

                if link and link.startswith("/url?q="):
                    try:
                        link = parse_qs(urlparse(link).query).get('q', [None])[0]
                    except Exception as e:
                        logging.warning(f"Could not parse redirect link: {link}. Error: {e}")
                        link = None

                if title and link:
                     search_results.append({
                         "title": title,
                         "snippet": " ".join(snippet.strip().split()) if snippet else "No snippet available.",
                         "link": link
                     })
                     page_results_count += 1


            logging.info(f"Scraped {page_results_count} results on this page.")

            next_page_link = soup.select_one("a#pnnext")
            if next_page_link and next_page_link.has_attr('href'):
                 next_page_url = next_page_link['href']
                 if next_page_url.startswith('/'):
                     next_page_url = "https://www.google.com" + next_page_url

                 parsed_next_url = urlparse(next_page_url)
                 query_params = parse_qs(parsed_next_url.query)
                 start_param_list = query_params.get('start')

                 if start_param_list and start_param_list[0].isdigit():
                     start = int(start_param_list[0])
                     logging.info(f"Found next page link. Updating start to {start}")
                 else:
                     logging.info("Next page link found, but 'start' parameter missing or invalid. Ending pagination.")
                     break
            else:
                logging.info("No next page link found. Ending pagination.")
                break

        except requests.exceptions.RequestException as e:
            logging.error(f"Requests error during scraping: {e}")
            break
        except Exception as e:
            logging.error(f"An unexpected error occurred during scraping: {e}", exc_info=True)
            break

    logging.info(f"Finished scraping loop. Collected {len(search_results)} results for query '{query}'.")
    return search_results


def perform_and_evaluate_topic_search_node(state: GraphState) -> Dict[str, Any]:
    """
    LangGraph node that performs web search and evaluates results for a single topic query.

    Takes the 'current_search_query' from the state, performs a web scrape
    (e.g., aiming for 10 results), evaluates the retrieved search results
    for relevance using the LLM, and appends any promising results (removing
    duplicates by link) to the 'cumulative_promising_results' list in the state.

    Args:
        state: The current state of the LangGraph, containing 'current_search_query',
               'raw_llm', 'cumulative_promising_results', and 'llm_raw_output'
               (needed for the original suggested areas for evaluation context).

    Returns:
        A dictionary containing an update to the state: 'cumulative_promising_results'
        with the promising search result dictionaries found for this topic, combined
        with results from previous topics.
    """
    user_email = state.get('user_email')
    current_search_query = state.get('current_search_query')
    raw_llm: ChatVertexAI = state.get('raw_llm')
    cumulative_promising_results = state.get('cumulative_promising_results', [])
    suggested_areas = state.get('llm_raw_output', {}).get('suggested_areas_of_improvement')


    if not current_search_query:
        logging.warning(f"[Node: perform_and_evaluate_topic_search] - No current search query available for user {user_email}. Skipping search for this topic.")
        return {"cumulative_promising_results": cumulative_promising_results}

    num_results_per_topic = 5
    logging.info(f"[Node: perform_and_evaluate_topic_search] - Performing web scrape for topic query '{current_search_query}' for user {user_email}, targeting {num_results_per_topic} results.")

    scraped_results = []
    try:
        scraped_results = scrape_google_search_results(current_search_query, num_results_per_topic)
        logging.info(f"[Node: perform_and_evaluate_topic_search] - Scraped {len(scraped_results)} results for topic query '{current_search_query}' for user {user_email}.")
    except Exception as scrape_e:
        logging.error(f"[Node: perform_and_evaluate_topic_search] - Error during web scraping for topic query '{current_search_query}' for user {user_email}: {scrape_e}", exc_info=True)


    newly_promising_results_for_this_topic = []

    if scraped_results and suggested_areas:
        logging.debug(f"[Node: perform_and_evaluate_topic_search] - Evaluating {len(scraped_results)} search results for topic query '{current_search_query}' for user: {user_email}")

        evaluation_prompt_template = """
            You are an expert at identifying online resources relevant to BigQuery SQL best practices.
            Based on the overall suggested areas for improvement for a user, evaluate the relevance of the following online resources (based on their titles and snippets), which were found for a specific related topic.

            Overall Suggested Areas for Improvement:
            {suggested_areas}

            Online Resources (Title and Snippet):
            {resources_snippets}

            For each resource, indicate whether it is highly relevant, moderately relevant, or not relevant to the overall suggested areas.
            Focus on resources that offer tutorials, documentation, best practices guides, or articles directly related to the suggested topics.
            Ignore marketing pages, irrelevant topics, or low-quality sources.

            Provide your evaluation as a JSON list of dictionaries, where each dictionary has 'link' (the URL of the resource) and 'relevance' ('highly relevant', 'moderately relevant', 'not relevant').
            Output only the JSON list, do not include any extra text or markdown formatting like ```json.
            Example output:
            [
                {{ "link": "url1", "relevance": "highly relevant" }},
                {{ "link": "url2", "relevance": "not relevant" }},
                ...
            ]
        """
        resources_snippets_formatted = ""
        for i, res in enumerate(scraped_results):
            resources_snippets_formatted += f"Resource {i+1}:\nTitle: {res.get('title', 'N/A')}\nSnippet: {res.get('snippet', 'N/A')}\nLink: {res.get('link', 'N/A')}\n---\n"

        eval_prompt = ChatPromptTemplate.from_template(evaluation_prompt_template)
        messages_for_eval = eval_prompt.format_messages(
            suggested_areas=suggested_areas,
            resources_snippets=resources_snippets_formatted
        )

        try:
            evaluation_response: AIMessage = raw_llm.invoke(messages_for_eval, {"recursion_limit": 40})
            response_content = evaluation_response.content

            if response_content.strip().startswith("```json"):
                 response_content = response_content.strip()[len("```json"):].strip()
            if response_content.strip().endswith("```"):
                 response_content = response_content.strip()[:-len("```")].strip()

            evaluations = []
            try:
                evaluations = json.loads(response_content)
                if not isinstance(evaluations, list):
                     logging.warning(f"[Node: perform_and_evaluate_topic_search] - LLM evaluation response was not a list for topic query '{current_search_query}' for user {user_email}. Type: {type(evaluations)}. Treating as empty.")
                     evaluations = []
                else:
                     evaluations = [e for e in evaluations if isinstance(e, dict) and 'link' in e and 'relevance' in e]

            except json.JSONDecodeError:
                logging.warning(f"[Node: perform_and_evaluate_topic_search] - Could not JSON parse LLM evaluation JSON response for topic query '{current_search_query}' for user {user_email}. Content: {response_content[:500]}...")
            except Exception as parse_e:
                 logging.warning(f"[Node: perform_and_evaluate_topic_search] - An unexpected error occurred during JSON parsing for evaluation for topic query '{current_search_query}' for user {user_email}: {parse_e}. Content: {response_content[:500]}...", exc_info=True)

            all_evaluated_links = {res.get('link'): res for res in scraped_results}
            for eval_item in evaluations:
                 link = eval_item.get('link')
                 relevance = eval_item.get('relevance', '').lower()
                 if link and relevance in ['highly relevant', 'moderately relevant']:
                     full_result = all_evaluated_links.get(link)
                     if full_result:
                          newly_promising_results_for_this_topic.append(full_result)
                          logging.debug(f"[Node: perform_and_evaluate_topic_search] - Identified promising result for topic '{current_search_query}' for user {user_email}: {link} (Relevance: {relevance})")
                     else:
                          logging.warning(f"[Node: perform_and_evaluate_topic_search] - LLM suggested a link ({link}) not in original scraped results for topic query '{current_search_query}' for user {user_email}. Skipping.")

            logging.info(f"[Node: perform_and_evaluate_topic_search] - Identified {len(newly_promising_results_for_this_topic)} new promising results for topic '{current_search_query}' for user {user_email}.")

        except Exception as eval_llm_e:
            logging.error(f"[Node: perform_and_evaluate_topic_search] - Error during LLM evaluation call for topic query '{current_search_query}' for user {user_email}: {eval_llm_e}", exc_info=True)

    else:
         if not scraped_results:
              logging.warning(f"[Node: perform_and_evaluate_topic_search] - No search results to evaluate for topic query '{current_search_query}' for user {user_email}.")
         if not suggested_areas:
              logging.warning(f"[Node: perform_and_evaluate_topic_search] - Suggested areas missing for evaluation for topic query '{current_search_query}' for user {user_email}.")

    updated_cumulative_promising_results_dict = {res['link']: res for res in cumulative_promising_results + newly_promising_results_for_this_topic}
    updated_cumulative_promising_results = list(updated_cumulative_promising_results_dict.values())

    logging.info(f"[Node: perform_and_evaluate_topic_search] - Total unique cumulative promising results for user {user_email}: {len(updated_cumulative_promising_results)}")

    return {"cumulative_promising_results": updated_cumulative_promising_results}


def finalize_urls_node(state: GraphState) -> Dict[str, Any]:
    """
    LangGraph node that performs a final selection of URLs from the cumulative promising list.

    This node uses the language model to select the most relevant and diverse set
    of URLs from the accumulated 'cumulative_promising_results' gathered across
    all topic searches, based on the 'suggested_areas_of_improvement'.
    It caps the final selection at a maximum of 4 URLs. After LLM selection,
    it performs a final accessibility validation on the chosen URLs.

    Args:
        state: The current state of the LangGraph, containing 'cumulative_promising_results',
               'llm_raw_output', and 'raw_llm'.

    Returns:
        A dictionary containing an update to the state: 'validated_urls' with the
        final list of accessible, relevant, and diverse URL strings (max 4).
    """
    user_email = state.get('user_email')
    promising_results = state.get('cumulative_promising_results', [])
    llm_raw_output = state.get('llm_raw_output')
    raw_llm: ChatVertexAI = state.get('raw_llm')

    suggested_areas = None
    if isinstance(llm_raw_output, dict):
        suggested_areas = llm_raw_output.get('suggested_areas_of_improvement')

    logging.debug(f"[Node: finalize_urls] - Finalizing URLs from {len(promising_results)} cumulative promising results for user: {user_email}")

    final_validated_urls = []
    max_final_urls = 4

    if not promising_results or not suggested_areas:
         logging.warning(f"[Node: finalize_urls] - No cumulative promising results ({len(promising_results)}) or suggestions to finalize URLs for user {user_email}. Returning empty validated URL list.")
         return {"validated_urls": []}

    selection_prompt_template = """
        Based on the following suggested areas for improvement for a user, select the {max_urls} MOST relevant and DIVERSE online study resources from the list provided below.
        Prioritize official documentation, high-quality articles, and tutorials.
        Ensure the selected resources cover different aspects of the suggested areas if possible, and avoid selecting too many links from the same domain unless they offer distinct valuable content.

        Suggested Areas for Improvement:
        {suggested_areas}

        Promising Online Resources (Title, Snippet, and Link):
        {promising_resources_formatted}

        Provide your selection as a JSON list of up to {max_urls} URLs. If fewer than {max_urls} relevant and diverse resources are available, provide only the ones you found. If none are suitable, provide an empty list.
        Output only the JSON list, do not include any extra text or markdown formatting like ```json.
        Example output:
        ["url1", "url2", "url3", ...]
    """

    promising_resources_formatted = ""
    for i, res in enumerate(promising_results):
        promising_resources_formatted += f"Resource {i+1}:\nTitle: {res.get('title', 'N/A')}\nSnippet: {res.get('snippet', 'N/A')}\nLink: {res.get('link', 'N/A')}\n---\n"

    selection_prompt = ChatPromptTemplate.from_template(selection_prompt_template)
    messages_for_selection = selection_prompt.format_messages(
        suggested_areas=suggested_areas,
        promising_resources_formatted=promising_resources_formatted,
        max_urls=max_final_urls
    )

    try:
        selection_response: AIMessage = raw_llm.invoke(messages_for_selection)
        response_content = selection_response.content

        if response_content.strip().startswith("```json"):
             response_content = response_content.strip()[len("```json"):].strip()
        if response_content.strip().endswith("```"):
             response_content = response_content.strip()[:-len("```")].strip()

        selected_urls_from_llm = []
        try:
            selected_urls_from_llm = json.loads(response_content)
            if not isinstance(selected_urls_from_llm, list):
                 logging.warning(f"[Node: finalize_urls] - LLM selection response was not a list for user {user_email}. Type: {type(selected_urls_from_llm)}. Content: {response_content[:500]}... Treating as empty.")
                 selected_urls_from_llm = []
            else:
                selected_urls_from_llm = [url for url in selected_urls_from_llm if isinstance(url, str) and url.strip()]


        except json.JSONDecodeError:
            logging.warning(f"[Node: finalize_urls] - Could not JSON parse LLM selection response for user {user_email}. Content: {response_content[:500]}...")
        except Exception as parse_e:
             logging.warning(f"[Node: finalize_urls] - An unexpected error occurred during JSON parsing for URL selection for user {user_email}: {parse_e}. Content: {response_content[:500]}...", exc_info=True)


        logging.debug(f"[Node: finalize_urls] - LLM selected {len(selected_urls_from_llm)} URLs for user {user_email} before validation.")

        logging.debug(f"[Node: finalize_urls] - Performing final accessibility validation on LLM-selected URLs for user {user_email}.")
        for url in selected_urls_from_llm:
            if len(final_validated_urls) >= max_final_urls:
                logging.debug(f"[Node: finalize_urls] - Reached max final URLs ({max_final_urls}). Stopping validation for user {user_email}.")
                break
            if validate_url(url):
                final_validated_urls.append(url)
                logging.debug(f"[Node: finalize_urls] - Final URL validated and included for user {user_email}: {url}")
            else:
                logging.debug(f"[Node: finalize_urls] - Final URL validation failed for user {user_email}: {url}. Excluding.")

        logging.info(f"[Node: finalize_urls] - Final list contains {len(final_validated_urls)} validated URLs for user {user_email}.")

    except Exception as node_e:
        logging.error(f"[Node: finalize_urls] - Error during LLM selection or validation for user {user_email}: {node_e}", exc_info=True)
        final_validated_urls = []

    return {"validated_urls": final_validated_urls}


def format_output_node(state: GraphState) -> Dict[str, Any]:
    """
    LangGraph node that formats the summarized report results for a user into the final output structure.

    This node takes the 'stats_summary_extraction' and 'suggested_areas_of_improvement'
    from the initial LLM call and the final 'validated_urls' list from the
    URL finalization node, combines them with the pre-calculated aggregated
    metrics from the state, and formats everything into a dictionary that
    matches the target output schema (AnalysisOutput model structure).
    This is the final output of the agent's workflow for a single user report.
    Ensures datetime objects are handled correctly for serialization.

    Args:
        state: The current state of the LangGraph, containing 'llm_raw_output',
               'validated_urls', 'window_start_time', 'window_end_time',
               'total_bytes_billed', 'analyzed_jobs_count', and 'grade'.

    Returns:
        A dictionary containing an update to the state: 'final_formatted_output'
        with the structured summarized report for the user and time window.
    """
    user_email = state.get('user_email')
    logging.debug(f"[Node: format_output] - Formatting output for user: {user_email}")
    llm_raw_output = state.get('llm_raw_output')
    validated_urls = state.get('validated_urls', [])
    window_start_time = state.get('window_start_time')
    window_end_time = state.get('window_end_time')
    total_bytes_billed = state.get('total_bytes_billed')
    analyzed_jobs_count = state.get('analyzed_jobs_count')
    grade = state.get('grade')
    llm_model = state.get('llm_model', 'unknown')

    stats_summary_extraction = None
    suggested_areas_of_improvement = None

    if isinstance(llm_raw_output, dict):
        logging.debug(f"[Node: format_output] - Using raw LLM output dictionary for summary and suggestions for user {user_email}")
        stats_summary_extraction = llm_raw_output.get('stats_summary_extraction')
        suggested_areas_of_improvement = llm_raw_output.get('suggested_areas_of_improvement')
    else:
         logging.warning(f"[Node: format_output] - llm_raw_output is not a dictionary for user {user_email}. Type: {type(llm_raw_output)}. Summaries and suggestions will be None.")


    window_days = None
    if window_start_time is not None and window_end_time is not None:
        if isinstance(window_start_time, datetime) and isinstance(window_end_time, datetime):
             delta = window_end_time - window_start_time
             window_days = delta.days
        else:
             logging.warning(f"[Node: format_output] - window_start_time or window_end_time is not a datetime object for user {user_email}. Cannot calculate window_days.")

    result_dict = {
        "analysis_timestamp": datetime.now(timezone.utc),
        "user_email": user_email,
        "window_start_time": window_start_time.replace(tzinfo=timezone.utc) if isinstance(window_start_time, datetime) and window_start_time.tzinfo is None else window_start_time,
        "window_end_time": window_end_time.replace(tzinfo=timezone.utc) if isinstance(window_end_time, datetime) and window_end_time.tzinfo is None else window_end_time,
        "window_days": window_days,
        "total_bytes_billed": total_bytes_billed,
        "analyzed_jobs_count": analyzed_jobs_count,
        "grade": grade,
        "stats_summary_extraction": stats_summary_extraction,
        "suggested_areas_of_improvement": suggested_areas_of_improvement,
        "online_study_urls": validated_urls
    }

    logging.debug(f"[Node: format_output] - Successfully formatted output for user {user_email}")
    return {"final_formatted_output": result_dict}


def build_analysis_agent(
    project_id: str,
    location: str,
    llm_model: str,
    agent_config_file: str
) -> Tuple[Any, ChatVertexAI, JsonOutputParser, str, str, ChatPromptTemplate]:
    """
    Initializes resources, loads configuration, builds, and compiles the LangGraph agent
    for user report summarization, including web search for URLs based on suggestions
    using iterative searches per topic and LLM evaluation for diversity.

    This function sets up the language model, loads the agent's configuration
    and prompt instructions, defines the LangGraph workflow nodes and edges,
    and compiles the graph. The workflow includes calling the LLM for summary
    and suggestions, extracting topics from suggestions using LLM, iterating
    through each topic to generate a specific search query, performing web
    scraping and evaluation for that topic, accumulating promising results,
    and finally finalizing the URLs from the cumulative list and formatting the output.

    Args:
        project_id: Google Cloud project ID where Vertex AI is available.
        location: Google Cloud location (region) for Vertex AI.
        llm_model: The name or identifier of the Vertex AI language model to use
                   (e.g., 'gemini-1.5-flash-001').
        agent_config_file: Filesystem path to the agent's JSON configuration file.
                           This file must contain 'instructions' and 'output_schema'.

    Returns:
        A tuple containing:
            - The compiled LangGraph agent instance (Any), ready to be invoked.
            - The initialized ChatVertexAI LLM instance (ChatVertexAI).
            - The initialized JsonOutputParser instance (JsonOutputParser).
            - The name of the LLM model used (str).
            - The output schema string (str).
            - The ChatPromptTemplate instance (ChatPromptTemplate) used for
              the initial LLM call.

    Raises:
        ValueError: If any required input arguments are missing or invalid, or if
                    essential configuration keys are missing.
        FileNotFoundError: If the agent configuration file is not found at the
                           specified path.
        Exception: For any other errors encountered during client initialization,
                   LLM setup, or graph compilation.
    """
    logging.info("Starting user report summarization agent build process (with iterative topic search and LLM diversity check)...")

    if not all([project_id, location, llm_model, agent_config_file]):
        missing_args = {
            "project_id": project_id,
            "location": location,
            "llm_model": llm_model,
            "agent_config_file": agent_config_file
        }
        missing = [arg_name for arg_name, val in missing_args.items() if not val]
        raise ValueError(f"Missing required arguments for building agent: {missing}")


    try:
        logging.info("Loading agent configuration...")
        agent_config = load_agent_config(agent_config_file)

        logging.info(f"Initializing ChatVertexAI LLM with model: {llm_model}")
        raw_llm = ChatVertexAI(
            project=project_id,
            location=location,
            model_name=llm_model,
            temperature=agent_config.get("model_parameters", {}).get("temperature", 0.1),
            # max_output_tokens=agent_config.get("model_parameters", {}).get("max_output_tokens", 1024) # Consider if needed
        )
        logging.info("ChatVertexAI LLM initialized.")

        output_schema_from_config = agent_config.get("output_schema")
        if not output_schema_from_config:
             raise ValueError("Agent configuration must contain 'output_schema'.")
        output_schema_str = json.dumps(output_schema_from_config, indent=2)

        logging.info("Building prompt template from config...")
        prompt = build_prompt_template(
             agent_config.get("instructions", "Analyze the provided BigQuery job data."),
             output_schema_str,
             agent_config.get('examples', [])
        )
        logging.info("Prompt template built.")

        logging.info("Defining LLM output parser...")
        output_parser = JsonOutputParser(pydantic_object=AnalysisOutput)
        logging.info("LLM output parser defined.")

        logging.info("Defining LangGraph workflow...")
        workflow = StateGraph(GraphState)

        # --- Add Nodes to the workflow ---
        workflow.add_node("call_llm_summarize_suggest", call_llm_summarize_suggest_node)
        workflow.add_node("extract_topics", extract_topics_node)
        workflow.add_node("initialize_topic_iteration", initialize_topic_iteration_node)
        workflow.add_node("process_next_topic", process_next_topic_node)
        workflow.add_node("generate_topic_search_query", generate_search_query_node)
        workflow.add_node("perform_and_evaluate_topic_search", perform_and_evaluate_topic_search_node)
        workflow.add_node("finalize_urls", finalize_urls_node)
        workflow.add_node("format_output", format_output_node)

        # --- Define Entry Point ---
        workflow.set_entry_point("call_llm_summarize_suggest")

        # --- Define Edges (Transitions) ---
        workflow.add_edge("call_llm_summarize_suggest", "extract_topics")
        workflow.add_edge("extract_topics", "initialize_topic_iteration")

        workflow.add_conditional_edges(
            "initialize_topic_iteration",  
            topic_router_node,           
            {
                "process_topic": "process_next_topic",   
                "finish_iteration": "finalize_urls",     
            }
        )

        workflow.add_edge("process_next_topic", "generate_topic_search_query")
        workflow.add_edge("generate_topic_search_query", "perform_and_evaluate_topic_search")

        workflow.add_conditional_edges(
            "perform_and_evaluate_topic_search", 
            topic_router_node,                  
            {
                "process_topic": "process_next_topic", 
                "finish_iteration": "finalize_urls", 
            }
        )

        workflow.add_edge("finalize_urls", "format_output")
        workflow.add_edge("format_output", END)

        # --- End Define Edges ---
        logging.info("Compiling LangGraph workflow...")
        compiled_agent = workflow.compile()
        logging.info("LangGraph workflow compiled successfully.")

        logging.info("Resource and agent build process completed.")

        return compiled_agent, raw_llm, output_parser, llm_model, output_schema_str, prompt

    except Exception as e:
        logging.critical(f"Fatal error during resource and agent build: {e}", exc_info=True)
        raise